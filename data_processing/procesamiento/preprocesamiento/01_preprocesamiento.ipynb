{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abcb90f",
   "metadata": {},
   "source": [
    "# **PREPROCESAMIENTO DE TEXTOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc8abd",
   "metadata": {},
   "source": [
    "## **PARA EMBEDDINGS NO CONTEXTUALES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53026964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "csv_path = r\"..\\..\\finnhubAPI\\data\\porEmpresas\\definitivos\\INDEX_ALL_scrapped_filtrado.csv\"\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccff16e",
   "metadata": {},
   "source": [
    "Lo primero es limpiar símbolos y distintas cosas que el modelo pueda malinterpretar. Observando los textos de los articulos, hemos detectado que se cuelan algunos simbolos como \">\" \"<\" o comillas,...Hemos tomado la decisión de eliminarlas para un mejor funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114d233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"textcat\"])\n",
    "tqdm.pandas()\n",
    "\n",
    "STOPWORDS = set(nlp.Defaults.stop_words)\n",
    "PLACEHOLDER_RE = re.compile(r\"__\\w+__\")\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "BOILERPLATE_RE = re.compile(\n",
    "    r\"(^read more:.*$|^story continues.*$|copyright\\s*©.*$)\",\n",
    "    flags=re.IGNORECASE | re.MULTILINE,\n",
    ")\n",
    "CASHTAG_RE = re.compile(r\"\\$([A-Za-z]{1,10})\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489d248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_rules(text: str) -> str:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    text = (text.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "                 .replace(\"–\", \"-\").replace(\"—\", \"-\")).lower()\n",
    "    t = BOILERPLATE_RE.sub(\"\", text)\n",
    "    t = HTML_TAG_RE.sub(\" \", t)\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = re.sub(CASHTAG_RE, \"__TICKER__\", t)\n",
    "\n",
    "    # placeholders financieros\n",
    "    t = re.sub(r'\\bQ([1-4])\\b', r'__QTR\\1__', t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r'\\b[+-]?\\d[\\d,\\.]*\\s*%(?=\\W|$)', '__PERCENT__', t)\n",
    "    t = re.sub(r'\\b[+-]?\\d[\\d,\\.]*\\s*percent(?=\\W|$)', '__PERCENT__', t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r'\\b[+-]?\\d[\\d,\\.]*\\s*per\\s+cent(?=\\W|$)', '__PERCENT__', t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r'(\\$|€|£)\\s*\\d[\\d,\\.]*\\s*(?:bn|b|m|k)?\\b', '__MONEY__', t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r'\\b\\d[\\d,\\.]*\\s*(million|billion|trillion|bn|m)\\b', '__AMOUNT__', t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r'\\b(19|20)\\d{2}\\b', '__YEAR__', t)\n",
    "    t = re.sub(r'\\b((jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*\\s+\\d{1,2})\\b',\n",
    "               '__DATE__', t, flags=re.IGNORECASE)\n",
    "    t = re.sub(r'\\b(\\d{1,2}\\s+(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec)[a-z]*)\\b',\n",
    "               '__DATE__', t, flags=re.IGNORECASE)\n",
    "    # números genéricos al final\n",
    "    t = re.sub(r'(?<![A-Za-z_])\\b\\d[\\d,\\.]*\\b(?![A-Za-z_])', '__NUM__', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87e9202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spacy_clean_strong(doc) -> str:\n",
    "    out = []\n",
    "    for tok in doc:\n",
    "        if tok.is_punct or tok.is_space:\n",
    "            continue\n",
    "        # conservar placeholders tal cual\n",
    "        if PLACEHOLDER_RE.fullmatch(tok.text):\n",
    "            out.append(tok.text); continue\n",
    "        if tok.text == \"%\":\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower()\n",
    "        if lemma == \"percent\":\n",
    "            out.append(\"__PERCENT__\"); continue\n",
    "        if tok.text.lower() == \"data\" and lemma == \"datum\":\n",
    "            lemma = \"data\"\n",
    "        if lemma in STOPWORDS:\n",
    "            continue\n",
    "        if lemma.isalpha() and len(lemma) > 2:\n",
    "            out.append(lemma)\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d978a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5160/5160 [01:59<00:00, 43.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      article_text                                                                          text_nc\n",
      "0  The UK jobs market continues to show signs of weakness, with pay growth slow...  job market continue sign weakness pay growth slowing unemployment edge highe...\n",
      "1  New data from the Department from Work and Pensions on the working patterns ...  new data department work pension work pattern people age num work life long ...\n",
      "2  Asking for workplace accommodations is often easier said than done. Many peo...  ask workplace accommodation easy people worry needy incompetent result conti...\n",
      "\n",
      "Longitud media (tokens aprox.): 235.75348837209302\n",
      "Guardado: processData.csv\n"
     ]
    }
   ],
   "source": [
    "base_col = \"article_text\"\n",
    "assert base_col in df.columns, f\"No existe la columna {base_col}\"\n",
    "\n",
    "df[\"text_nc_step1\"] = df[base_col].apply(pre_rules)\n",
    "df[\"text_nc\"] = df[\"text_nc_step1\"].progress_apply(lambda x: spacy_clean_strong(nlp(x)))\n",
    "\n",
    "print(df[[\"article_text\", \"text_nc\"]].head(3).to_string(max_colwidth=80))\n",
    "print(\"\\nLongitud media (tokens aprox.):\",\n",
    "      df[\"text_nc\"].str.split().map(len).replace(0, pd.NA).mean())\n",
    "\n",
    "out_path = \"processData.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"Guardado:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52312a",
   "metadata": {},
   "source": [
    "## **PARA EMBEDDINGS CONTEXTUALES**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
