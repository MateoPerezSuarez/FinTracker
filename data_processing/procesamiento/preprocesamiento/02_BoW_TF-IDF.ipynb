{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ae5783",
   "metadata": {},
   "source": [
    "# **M√âTODOS DE REPRESENTACI√ìN TRADICIONALES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fad7b7",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9d93561",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"..\\..\\finnhubAPI\\data\\procesadosINDEX_ALL_preprocessed.csv\"\n",
    "df.to_csv(save_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30145996",
   "metadata": {},
   "source": [
    "### PRUEBA MONTAR EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "535a2237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape TF-IDF: (5160, 104149)\n",
      "Tama√±o del vocabulario: 104149\n",
      "‚úÖ Guardado en: C:\\Users\\mpsua\\OneDrive\\Escritorio\\ud\\CUARTO\\Primer_Cuatri\\PLN\\Proyecto\\FinTracker\\data_processing\\procesamiento\\preprocesamiento\\embeddings_tfidf\n"
     ]
    }
   ],
   "source": [
    "# === TF-IDF: construcci√≥n y exportaci√≥n de artefactos ===\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from joblib import dump\n",
    "\n",
    "TEXT_COL = \"preprocessed_text\"    # tu columna ya procesada\n",
    "SAVE_DIR = Path(\"embeddings_tfidf\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TFIDF_CFG = {\n",
    "    \"ngram_range\": (1, 2),        # unigrams + bigrams\n",
    "    \"min_df\": 3,\n",
    "    \"max_df\": 0.90,\n",
    "    \"max_features\": 120_000,      # sube/baja seg√∫n RAM\n",
    "    \"norm\": \"l2\",\n",
    "    \"use_idf\": True,\n",
    "    \"smooth_idf\": True,\n",
    "    \"sublinear_tf\": False,\n",
    "    \"dtype\": np.float32,\n",
    "}\n",
    "\n",
    "df_embed = df[[TEXT_COL]].dropna()\n",
    "df_embed = df_embed[df_embed[TEXT_COL].astype(str).str.strip().astype(bool)]\n",
    "texts = df_embed[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "vec = TfidfVectorizer(**TFIDF_CFG)\n",
    "X = vec.fit_transform(texts)                # <--- ESTE es tu embedding (sparse)\n",
    "\n",
    "print(\"Shape TF-IDF:\", X.shape)             # (n_docs, n_features)\n",
    "print(\"Tama√±o del vocabulario:\", len(vec.vocabulary_))\n",
    "\n",
    "# Guarda matriz y artefactos\n",
    "sparse.save_npz(SAVE_DIR / \"tfidf_X.npz\", X)\n",
    "dump(vec, SAVE_DIR / \"tfidf_vectorizer.joblib\")\n",
    "\n",
    "# Vocabulario e IDF (√∫til para inspecci√≥n/reproducibilidad)\n",
    "terms = vec.get_feature_names_out()\n",
    "idf = vec.idf_\n",
    "idf_df = pd.DataFrame({\"term\": terms, \"idf\": idf}).sort_values(\"idf\", ascending=False)\n",
    "idf_df.to_csv(SAVE_DIR / \"tfidf_idf.csv\", index=False)\n",
    "\n",
    "# ‚öôÔ∏è Crear versi√≥n JSON serializable del config\n",
    "TFIDF_CFG_JSON = dict(TFIDF_CFG)\n",
    "TFIDF_CFG_JSON[\"dtype\"] = np.dtype(TFIDF_CFG[\"dtype\"]).name  # convierte float32 en \"float32\"\n",
    "TFIDF_CFG_JSON[\"ngram_range\"] = list(TFIDF_CFG[\"ngram_range\"])  # convierte tuple ‚Üí list\n",
    "\n",
    "with open(SAVE_DIR / \"tfidf_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(TFIDF_CFG_JSON, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Guardado en:\", SAVE_DIR.resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe88ca",
   "metadata": {},
   "source": [
    "## EVALUACI√ìN TRADICIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c74454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò Shape TF-IDF: (5160, 104149) (docs x vocab)\n",
      "üìä Sparsity: 0.002787  (0.2787% de celdas no nulas)\n",
      "üî† Tama√±o del vocabulario: 104,149\n",
      "\n",
      "üî• 10 t√©rminos m√°s comunes (bajo IDF):\n",
      "year, company, num, ai, __percent__, market, stock, new, share, high\n",
      "\n",
      "üßä 10 t√©rminos m√°s raros (alto IDF):\n",
      "zuckerberg year, zuckerberg widely, zuckerberg wednesday, zuckerberg time, zuckerberg talk, gpu operate, zuckerberg recognition, zuckerberg recently, zuckerberg quickly, zuckerberg pull\n",
      "\n",
      "üì∞ Top 10 t√©rminos del documento 0:\n",
      "autumn budget        0.19996\n",
      "ons                  0.19609\n",
      "autumn               0.18688\n",
      "unemployment         0.17604\n",
      "fall num             0.17263\n",
      "estimate number      0.15747\n",
      "number               0.15146\n",
      "num july             0.14965\n",
      "vacancy              0.14668\n",
      "continue sign        0.14668\n",
      "\n",
      "ü§ù Similitud coseno entre documento 0 y 1: 0.0351\n",
      "\n",
      "‚úÖ Informe de embedding guardado en: embeddings_tfidf\\embedding_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# === Evaluaci√≥n exploratoria del embedding TF-IDF ===\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "n_docs, n_features = X.shape\n",
    "sparsity = X.nnz / (n_docs * n_features)  # proporci√≥n de elementos distintos de 0\n",
    "print(f\"üìò Shape TF-IDF: {X.shape} (docs x vocab)\")\n",
    "print(f\"üìä Sparsity: {sparsity:.6f}  ({sparsity*100:.4f}% de celdas no nulas)\")\n",
    "print(f\"üî† Tama√±o del vocabulario: {len(vec.vocabulary_):,}\")\n",
    "\n",
    "# --- 2Ô∏è‚É£ T√©rminos m√°s comunes / m√°s raros ---\n",
    "terms = np.array(vec.get_feature_names_out())\n",
    "idf = vec.idf_\n",
    "\n",
    "common_terms = terms[np.argsort(idf)[:10]]\n",
    "rare_terms   = terms[np.argsort(-idf)[:10]]\n",
    "\n",
    "print(\"\\nüî• 10 t√©rminos m√°s comunes (bajo IDF):\")\n",
    "print(\", \".join(common_terms))\n",
    "print(\"\\nüßä 10 t√©rminos m√°s raros (alto IDF):\")\n",
    "print(\", \".join(rare_terms))\n",
    "\n",
    "# --- 3Ô∏è‚É£ Top t√©rminos de un documento espec√≠fico ---\n",
    "def top_terms_for_doc(i, vec, X, n=10):\n",
    "    row = X[i].toarray().ravel()\n",
    "    top_idx = row.argsort()[::-1][:n]\n",
    "    return list(zip(terms[top_idx], row[top_idx]))\n",
    "\n",
    "idx_example = 0  # cambia el √≠ndice para otros documentos\n",
    "top_terms = top_terms_for_doc(idx_example, vec, X, n=10)\n",
    "print(f\"\\nüì∞ Top 10 t√©rminos del documento {idx_example}:\")\n",
    "for term, weight in top_terms:\n",
    "    print(f\"{term:<20} {weight:.5f}\")\n",
    "\n",
    "# --- 4Ô∏è‚É£ Similitud entre documentos ---\n",
    "if X.shape[0] > 1:\n",
    "    sim = cosine_similarity(X[0], X[1])[0,0]\n",
    "    print(f\"\\nü§ù Similitud coseno entre documento 0 y 1: {sim:.4f}\")\n",
    "\n",
    "# --- 5Ô∏è‚É£ (Opcional) Guardar resumen r√°pido ---\n",
    "with open(SAVE_DIR / \"embedding_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Shape: {X.shape}\\n\")\n",
    "    f.write(f\"Sparsity: {sparsity:.6f}\\n\")\n",
    "    f.write(\"Top comunes: \" + \", \".join(common_terms) + \"\\n\")\n",
    "    f.write(\"Top raros: \" + \", \".join(rare_terms) + \"\\n\")\n",
    "    f.write(f\"Ejemplo doc {idx_example}: \" +\n",
    "            \", \".join([t for t,_ in top_terms]) + \"\\n\")\n",
    "print(\"\\n‚úÖ Informe de embedding guardado en:\", SAVE_DIR / \"embedding_summary.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
